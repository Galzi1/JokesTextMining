{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining  pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's import a few free-open source tools to our convenience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_json(\"stupidstuff.json\")\n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature eng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of words\n",
    "- number of characters\n",
    "- number of sentences\n",
    "- number of punctuations\n",
    "- number of all CAPS-LOCK words\n",
    "- ratio of all CAPS-LOCK words to total words\n",
    "- number of stopwords\n",
    "- mean length of words\n",
    "- number of syntax errors\n",
    "- number of new-lines? /isn't sentence\n",
    "- specific punctuations: ?, !\n",
    "- repetition of words? / BoW-TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def words_counter(text):\n",
    "    return len(text.split())\n",
    "\n",
    "data['words_count'] = data['body'].apply(words_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characters_counter(text):\n",
    "    return len(text)\n",
    "data['characters_count'] = data['body'].apply(characters_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it?\n",
    "def sentences_counter(text): \n",
    "    return len(sent_tokenize(text))\n",
    "data['sentences_count'] = data['body'].apply(sentences_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "def punct_counter(text):\n",
    "    return count(text, set(string.punctuation))\n",
    "data['punct_count'] = data['body'].apply(punct_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it?\n",
    "def caps_counter(text):\n",
    "    return sum(1 for c in text if c.isupper())\n",
    "data['capitals_count'] = data['body'].apply(caps_counter)\n",
    "data['capitals_count'] = data['capitals_count'] - data['sentences_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_counter(text):\n",
    "    stops = [word for word in text if word in stopwords]\n",
    "    return len(stops)\n",
    "data['stopwords_counter'] = data['body'].apply(stopwords_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_length(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return (sum( map(len, text) ) / len(text))\n",
    "\n",
    "data['mean_len_word'] = data['body'].apply(lambda x: mean_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_counter(text): \n",
    "    return text.count(\".\")\n",
    "data['dot_counter'] = data['body'].apply(dot_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma_counter(text): \n",
    "    return text.count(\",\")\n",
    "data['comma_counter'] = data['body'].apply(comma_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_cleaner(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word.lower() for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "data['text_prepared'] = data['body'].apply(stopwords_cleaner)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "Bow = CountVectorizer(analyzer=stem_text)\n",
    "X_Bow = tfidf_vect.fit_transform(data['text_prepared'])\n",
    "\n",
    "X_features = pd.concat([data['body_len'], pd.DataFrame(X_Bow.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [20, 50, 100, 200, 500],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth' : [2,4,8,16],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'class_weight': [{1: 10, 0:1}, {1: 9, 0:1}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(random_state=42)\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5, scoring= scoreFunction, n_jobs=-1)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1=RandomForestClassifier(random_state=42, n_jobs=-1,\n",
    "                            max_features=CV_rfc.best_params_['max_features'],\n",
    "                            n_estimators= CV_rfc.best_params_['n_estimators'],\n",
    "                            max_depth=CV_rfc.best_params_['max_depth'],\n",
    "                            criterion=CV_rfc.best_params_['criterion'],\n",
    "                           class_weight=CV_rfc.best_params_['class_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1.fit(x_train, y_train)\n",
    "pred=rfc1.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,50,40) #y amount of samples, x len of each\n",
    "pp.hist(data['body_len'], bins)\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3]:\n",
    "    bins = np.linspace(0,50/(i**i),40)\n",
    "    pp.hist((data[data['label'] == 'Valid']['body_len'])**(1/i), bins, label = 'VALID', alpha = 0.5)\n",
    "    pp.hist((data[data['label'] == 'Spam']['body_len'])**(1/i), bins, label = 'SPAM', alpha = 0.5)\n",
    "    pp.legend(loc='upper left')\n",
    "    pp.title('transformation 1/{}'.format(str(i)))\n",
    "    pp.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
